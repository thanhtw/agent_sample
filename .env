# Ollama configuration
# Base URL for Ollama API - default to http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# Default model to use - llama3.2:1b is the default
DEFAULT_MODEL=llama3.2:1b

# Optional: Set specific models for each agent component
# If not specified, DEFAULT_MODEL will be used
GENERATIVE_MODEL=llama3.2:1b
REVIEW_MODEL=llama3.2:1b
SUMMARY_MODEL=llama3.2:1b
COMPARE_MODEL=llama3.2:1b

# Optional: Set temperature values for each agent component
# Higher values (0.7-1.0) produce more creative outputs, lower values (0.0-0.3) more deterministic
GENERATIVE_TEMPERATURE=0.7
REVIEW_TEMPERATURE=0.2
SUMMARY_TEMPERATURE=0.3
COMPARE_TEMPERATURE=0.2